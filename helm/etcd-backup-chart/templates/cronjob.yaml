apiVersion: batch/v2alpha1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  # Run every six hours.
  schedule: "0 */6 * * *"
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      # Cleaning jobs after 6h, same schedule period
      ttlSecondsAfterFinished: 21600
      # Job timeout
      activeDeadlineSeconds: 800
      template:
        spec:
          tolerations:
          # Tolerate master taint
          - key: node-role.kubernetes.io/master
            operator: Exists
            effect: NoSchedule
          # Container creates etcd backups.
          # Run container in host network mode on G8s masters
          # to be able to use 127.0.0.1 as etcd address.
          # For etcd v2 backups container should have access
          # to etcd data directory. To achive that,
          # mount /var/lib/etcd3 as a volume.
          nodeSelector:
            node-role.kubernetes.io/master: ""
          serviceAccountName: etcd-backup
          containers:
          - name: etcd-backup
            image: quay.io/giantswarm/etcd-backup:[[ .SHA ]]
            args:
            # backup guest clusters only on production instalations
            # testing installation can have many broken guest clusters
            - -guest-backup={{ not .Values.Installation.V1.Infra.TestingEnvironment }}
            - -prefix={{ .Values.Installation.V1.Infra.EtcdBackup.ClusterPrefix }}
            - -provider={{ .Values.Installation.V1.Provider.Kind }}
            - -etcd-v2-datadir=/var/lib/etcd
            - -etcd-v3-endpoints={{ .Values.Installation.V1.Infra.EtcdBackup.EtcdEndpoints }}
            - -etcd-v3-cacert=/certs/{{ .Values.Installation.V1.Infra.EtcdBackup.ClientCaCertFileName }}
            - -etcd-v3-cert=/certs/{{ .Values.Installation.V1.Infra.EtcdBackup.ClientCertFileName }}
            - -etcd-v3-key=/certs/{{ .Values.Installation.V1.Infra.EtcdBackup.ClientKeyFileName }}
            - -aws-s3-bucket={{ .Values.Installation.V1.Infra.EtcdBackup.S3Bucket }}
            - -aws-s3-region={{ .Values.Installation.V1.Infra.EtcdBackup.S3Region }}
            {{- if and (.Values.Installation.V1.Infra.EtcdBackup.PushgatewayJob | default "") (.Values.Installation.V1.Infra.EtcdBackup.PushgatewayURL | default "") }}
            - -prometheus-url={{ .Values.Installation.V1.Infra.EtcdBackup.PushgatewayURL | quote }}
            - -prometheus-job={{ .Values.Installation.V1.Infra.EtcdBackup.PushgatewayJob | quote }}
            {{- end }}
            volumeMounts:
            - mountPath: /var/lib/etcd
              name: etcd-datadir
            - mountPath: /certs
              name: etcd-certs
            env:
              - name: ETCDBACKUP_AWS_ACCESS_KEY
                valueFrom:
                  secretKeyRef:
                    name: etcd-backup
                    key: ETCDBACKUP_AWS_ACCESS_KEY
              - name: ETCDBACKUP_AWS_SECRET_KEY
                valueFrom:
                  secretKeyRef:
                    name: etcd-backup
                    key: ETCDBACKUP_AWS_SECRET_KEY
              - name: ETCDBACKUP_PASSPHRASE
                valueFrom:
                  secretKeyRef:
                    name: etcd-backup
                    key: ETCDBACKUP_PASSPHRASE
          volumes:
          - name: etcd-datadir
            hostPath:
              path: {{ .Values.Installation.V1.Infra.EtcdBackup.EtcdDataDir }}
          - name: etcd-certs
            hostPath:
              path: {{ .Values.Installation.V1.Infra.EtcdBackup.ClientCertsDir }}
          # Do not restart pod, job takes care on restarting failed pod.
          restartPolicy: Never
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          imagePullSecrets:
          - name: etcd-backup-pull-secret
